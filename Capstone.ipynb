{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKtwBitLCkzPNCAGMTg+Ab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fdogbe1/Capstone/blob/main/Capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62mf91vxd7af",
        "outputId": "986f2148-6c14-488e-8b11-796535f325f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = \"/content/drive/My Drive/Capstone\"\n",
        "\n",
        "# Listing all files in my Capstone folder\n",
        "files = os.listdir(folder_path)\n",
        "print(files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXNunm8-eUcE",
        "outputId": "286e260f-aa21-480e-bd5f-5e181af55d46"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ebd_US_rosgoo_199501_202501_relDec-2024.txt', 'ebd_US_amwpel_199501_202501_relDec-2024.txt', 'ebd_US_balori_199501_202501_relDec-2024.txt', 'ebd_US_rthhum_relDec-2024.txt', 'ebd_US_mallar2_199501_202501_relDec-2024.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Defining the folder path\n",
        "folder_path = \"/content/drive/My Drive/Capstone\"\n",
        "\n",
        "# Getting a list of all .txt files in my folder\n",
        "file_list = glob.glob(folder_path + \"/*.txt\")\n",
        "\n",
        "# Reading and concatenating all text files\n",
        "df_list = []\n",
        "for file in file_list:\n",
        "    try:\n",
        "        df = pd.read_csv(file, delimiter=\"\\t\", header=None, dtype=str, on_bad_lines='skip', low_memory=False)\n",
        "        df_list.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file}: {e}\")\n",
        "\n",
        "# Concatenating all DataFrames\n",
        "combined_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Displaying the first few rows\n",
        "print(\"Successfully loaded all files!\")\n",
        "print(combined_df.head())\n",
        "\n",
        "# Saving the combined DataFrame\n",
        "combined_df.to_csv(\"/content/drive/My Drive/Capstone/combined_output.csv\", index=False)\n",
        "print(\"Concatenated file saved successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M-Mcmbtec80",
        "outputId": "964ec07f-8f6d-40f5-a7de-86e11a4ac615"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded all files!\n",
            "                                               0                           1   \\\n",
            "0                        GLOBAL UNIQUE IDENTIFIER            LAST EDITED DATE   \n",
            "1  URN:CornellLabOfOrnithology:EBIRD:OBS333593933         2016-06-22 18:22:50   \n",
            "2  URN:CornellLabOfOrnithology:EBIRD:OBS136321732         2014-02-26 21:46:48   \n",
            "3  URN:CornellLabOfOrnithology:EBIRD:OBS539131197  2024-04-17 21:41:47.133016   \n",
            "4  URN:CornellLabOfOrnithology:EBIRD:OBS136325189         2014-02-26 21:46:48   \n",
            "\n",
            "                2         3                 4             5                6   \\\n",
            "0  TAXONOMIC ORDER  CATEGORY  TAXON CONCEPT ID   COMMON NAME  SCIENTIFIC NAME   \n",
            "1              261   species  avibase-7A24F57F  Ross's Goose     Anser rossii   \n",
            "2              261   species  avibase-7A24F57F  Ross's Goose     Anser rossii   \n",
            "3              261   species  avibase-7A24F57F  Ross's Goose     Anser rossii   \n",
            "4              261   species  avibase-7A24F57F  Ross's Goose     Anser rossii   \n",
            "\n",
            "                       7                           8            9   ...  \\\n",
            "0  SUBSPECIES COMMON NAME  SUBSPECIES SCIENTIFIC NAME  EXOTIC CODE  ...   \n",
            "1                     NaN                         NaN          NaN  ...   \n",
            "2                     NaN                         NaN          NaN  ...   \n",
            "3                     NaN                         NaN          NaN  ...   \n",
            "4                     NaN                         NaN          NaN  ...   \n",
            "\n",
            "                 40                    41                42         43  \\\n",
            "0  NUMBER OBSERVERS  ALL SPECIES REPORTED  GROUP IDENTIFIER  HAS MEDIA   \n",
            "1                 1                     1               NaN          0   \n",
            "2                 2                     0           G341171          0   \n",
            "3               NaN                     0               NaN          0   \n",
            "4                 2                     0           G341171          0   \n",
            "\n",
            "         44        45      46             47  \\\n",
            "0  APPROVED  REVIEWED  REASON  TRIP COMMENTS   \n",
            "1         1         1     NaN            NaN   \n",
            "2         1         1     NaN            NaN   \n",
            "3         1         1     NaN            NaN   \n",
            "4         1         1     NaN            NaN   \n",
            "\n",
            "                                                  48   49  \n",
            "0                                   SPECIES COMMENTS  NaN  \n",
            "1  like snow goose, but rounder head, shorter nec...  NaN  \n",
            "2  Rare bird alert sent us to a greenway in downt...  NaN  \n",
            "3  Details and photos on file UAF Museuem. April ...  NaN  \n",
            "4  Rare bird alert sent us to a greenway in downt...  NaN  \n",
            "\n",
            "[5 rows x 50 columns]\n",
            "Concatenated file saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Defining the list of states to filter\n",
        "selected_states = [\n",
        "    \"Alabama\", \"Arkansas\", \"Indiana\", \"Illinois\", \"Iowa\", \"Kentucky\",\n",
        "    \"Louisiana\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\",\n",
        "    \"Ohio\", \"Tennessee\", \"Wisconsin\"\n",
        "]\n",
        "\n",
        "# Converting state list to lowercase for case-insensitive comparison\n",
        "selected_states_lower = [state.lower() for state in selected_states]\n",
        "\n",
        "# File paths\n",
        "combined_file_path = \"/content/drive/My Drive/Capstone/combined_output.csv\"\n",
        "filtered_file_path = \"/content/drive/My Drive/Capstone/filtered_output.csv\"\n",
        "\n",
        "# Initializing counters\n",
        "total_rows_before = 0\n",
        "total_rows_after = 0\n",
        "\n",
        "# Processing the dataset in chunks to handle large files efficiently\n",
        "with pd.read_csv(combined_file_path, dtype=str, low_memory=False, chunksize=100000, header=1) as reader:\n",
        "\n",
        "    first_chunk = True  # Flag for writing headers in the output file\n",
        "\n",
        "    for chunk in reader:\n",
        "        total_rows_before += len(chunk)  # Count total rows before filtering\n",
        "\n",
        "        # Ensuring the 'STATE' column exists\n",
        "        if \"STATE\" not in chunk.columns:\n",
        "            raise ValueError(f\"Column 'STATE' not found. Available columns: {chunk.columns}\")\n",
        "\n",
        "        # Converting 'State' column to lowercase for case-insensitive matching\n",
        "        chunk[\"STATE\"] = chunk[\"STATE\"].str.lower()\n",
        "\n",
        "        # Filtering dataset to only include selected states\n",
        "        chunk_filtered = chunk[chunk[\"STATE\"].isin(selected_states_lower)]\n",
        "        total_rows_after += len(chunk_filtered)  # Count total rows after filtering\n",
        "\n",
        "        # Appending the filtered data to the output file\n",
        "        chunk_filtered.to_csv(filtered_file_path, mode='a', index=False, header=first_chunk)\n",
        "        first_chunk = False  # Ensure headers are only written once\n",
        "\n",
        "# Print total observations before and after filtering\n",
        "print(f\"Total observations before filtering: {total_rows_before}\")\n",
        "print(f\"Total observations after filtering: {total_rows_after}\")\n",
        "\n",
        "# Print confirmation message\n",
        "print(f\"Filtered dataset saved successfully at: {filtered_file_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKv0x1nuj_Gk",
        "outputId": "1fa9e960-ddb7-4636-9555-14903524e484"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total observations before filtering: 9741164\n",
            "Total observations after filtering: 3382801\n",
            "Filtered dataset saved successfully at: /content/drive/My Drive/Capstone/filtered_output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define file paths\n",
        "filtered_file_path = \"/content/drive/My Drive/Capstone/filtered_output.csv\"\n",
        "final_subset_file_path = \"/content/drive/My Drive/Capstone/final_subset.csv\"\n",
        "\n",
        "# Define the required columns\n",
        "required_columns = [\n",
        "    \"GLOBAL UNIQUE IDENTIFIER\", \"COMMON NAME\", \"SCIENTIFIC NAME\", \"OBSERVATION COUNT\",\n",
        "    \"STATE\", \"COUNTY\", \"COUNTY CODE\", \"LOCALITY ID\", \"LOCALITY TYPE\",\n",
        "    \"LATITUDE\", \"LONGITUDE\", \"OBSERVATION DATE\"\n",
        "]\n",
        "\n",
        "# Load the filtered dataset\n",
        "df_filtered = pd.read_csv(filtered_file_path, dtype=str, low_memory=False)\n",
        "\n",
        "# Ensure required columns exist\n",
        "missing_cols = [col for col in required_columns if col not in df_filtered.columns]\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Missing columns: {missing_cols}. Available columns: {df_filtered.columns}\")\n",
        "\n",
        "# Keep only the required columns\n",
        "df_subset = df_filtered[required_columns]\n",
        "\n",
        "# Save the final subset to a new file\n",
        "df_subset.to_csv(final_subset_file_path, index=False)\n",
        "\n",
        "# Print confirmation message\n",
        "print(f\"Subsetted dataset saved successfully at: {final_subset_file_path}\")\n",
        "print(f\"Number of rows in subset: {df_subset.shape[0]}\")\n",
        "print(f\"Number of columns in subset: {df_subset.shape[1]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJm6czHjy6yc",
        "outputId": "f8bae732-b44d-48c3-b69f-749c8bf6638f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subsetted dataset saved successfully at: /content/drive/My Drive/Capstone/final_subset.csv\n",
            "Number of rows in subset: 6267848\n",
            "Number of columns in subset: 12\n"
          ]
        }
      ]
    }
  ]
}